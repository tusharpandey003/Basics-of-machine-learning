{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMqmvVhH+PeBwxy2ND7kBUu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Overview\n","The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called hyperparameter tuning or hypertuning.\n","\n","Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program."],"metadata":{"id":"91bicYZJwbuw"}},{"cell_type":"code","source":["! pip install -q -U keras-tuner"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jm5RJkNArCkE","executionInfo":{"status":"ok","timestamp":1713787191623,"user_tz":-330,"elapsed":8583,"user":{"displayName":"Tushar Pandey","userId":"02510744473828278127"}},"outputId":"7f0f6093-d3c3-4dc2-f2ec-518164b161f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/129.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y82yqNNKqke-"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import keras_tuner as kt"]},{"cell_type":"code","source":["(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EyM0w35XrW2l","executionInfo":{"status":"ok","timestamp":1713787210798,"user_tz":-330,"elapsed":724,"user":{"displayName":"Tushar Pandey","userId":"02510744473828278127"}},"outputId":"cf760d54-fd23-4d02-cbf5-3e0fe659fef8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","29515/29515 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26421880/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","5148/5148 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4422102/4422102 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","source":["# Normalize pixel values between 0 and 1"],"metadata":{"id":"0qaq1w0vwlvH"}},{"cell_type":"code","source":["img_train = img_train.astype('float32') / 255.0\n","img_test = img_test.astype('float32') / 255.0"],"metadata":{"id":"voU7VBiSrYD3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def model_builder(hp):\n","  model = keras.Sequential()\n","  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n","\n","  # Tune the number of units in the first Dense layer\n","  # Choose an optimal value between 32-512\n","  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n","  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n","  model.add(keras.layers.Dense(10))\n","\n","  # Tune the learning rate for the optimizer\n","  # Choose an optimal value from 0.01, 0.001, or 0.0001\n","  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","\n","  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n","                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                metrics=['accuracy'])\n","\n","  return model"],"metadata":{"id":"yoWVxHqnra7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tuner = kt.Hyperband(model_builder,\n","                     objective='val_accuracy',\n","                     max_epochs=10,\n","                     factor=3,\n","                     directory='my_dir',\n","                     project_name='intro_to_kt')"],"metadata":{"id":"Br9zIF1trmmc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"],"metadata":{"id":"6j8TJ09zrpqO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get the optimal hyperparameters"],"metadata":{"id":"a7LgmGphwvfh"}},{"cell_type":"code","source":["tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n","best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","print(f\"\"\"\n","The hyperparameter search is complete. The optimal number of units in the first densely-connected\n","layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n","is {best_hps.get('learning_rate')}.\n","\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k7PVEJQ5rsYK","executionInfo":{"status":"ok","timestamp":1713788701997,"user_tz":-330,"elapsed":1307,"user":{"displayName":"Tushar Pandey","userId":"02510744473828278127"}},"outputId":"af0e95cd-b73f-4d20-cbd4-7792a732c1ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","The hyperparameter search is complete. The optimal number of units in the first densely-connected\n","layer is 256 and the optimal learning rate for the optimizer\n","is 0.001.\n","\n"]}]},{"cell_type":"markdown","source":["# Build the model with the optimal hyperparameters and train it on the data for 50 epochs"],"metadata":{"id":"6oJPd46-wzWH"}},{"cell_type":"code","source":["model = tuner.hypermodel.build(best_hps)\n","history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n","\n","val_acc_per_epoch = history.history['val_accuracy']\n","best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n","print('Best epoch: %d' % (best_epoch,))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfgwZDooxwMK","executionInfo":{"status":"ok","timestamp":1713789387416,"user_tz":-330,"elapsed":503117,"user":{"displayName":"Tushar Pandey","userId":"02510744473828278127"}},"outputId":"99990054-9564-4aea-e050-0d440b51f52e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1500/1500 [==============================] - 11s 7ms/step - loss: 0.5031 - accuracy: 0.8207 - val_loss: 0.4301 - val_accuracy: 0.8443\n","Epoch 2/50\n","1500/1500 [==============================] - 10s 6ms/step - loss: 0.3736 - accuracy: 0.8649 - val_loss: 0.3585 - val_accuracy: 0.8724\n","Epoch 3/50\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.3352 - accuracy: 0.8779 - val_loss: 0.3558 - val_accuracy: 0.8668\n","Epoch 4/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.3096 - accuracy: 0.8859 - val_loss: 0.3562 - val_accuracy: 0.8720\n","Epoch 5/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2913 - accuracy: 0.8918 - val_loss: 0.3369 - val_accuracy: 0.8797\n","Epoch 6/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2741 - accuracy: 0.8974 - val_loss: 0.3301 - val_accuracy: 0.8846\n","Epoch 7/50\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.2618 - accuracy: 0.9026 - val_loss: 0.3164 - val_accuracy: 0.8878\n","Epoch 8/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2492 - accuracy: 0.9061 - val_loss: 0.3177 - val_accuracy: 0.8882\n","Epoch 9/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2392 - accuracy: 0.9093 - val_loss: 0.3277 - val_accuracy: 0.8842\n","Epoch 10/50\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.2305 - accuracy: 0.9143 - val_loss: 0.3273 - val_accuracy: 0.8883\n","Epoch 11/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2184 - accuracy: 0.9188 - val_loss: 0.3323 - val_accuracy: 0.8858\n","Epoch 12/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2120 - accuracy: 0.9202 - val_loss: 0.3222 - val_accuracy: 0.8882\n","Epoch 13/50\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.2028 - accuracy: 0.9244 - val_loss: 0.3256 - val_accuracy: 0.8888\n","Epoch 14/50\n","1500/1500 [==============================] - 10s 6ms/step - loss: 0.1972 - accuracy: 0.9261 - val_loss: 0.3506 - val_accuracy: 0.8828\n","Epoch 15/50\n","1500/1500 [==============================] - 10s 6ms/step - loss: 0.1938 - accuracy: 0.9262 - val_loss: 0.3296 - val_accuracy: 0.8923\n","Epoch 16/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1836 - accuracy: 0.9312 - val_loss: 0.3336 - val_accuracy: 0.8904\n","Epoch 17/50\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.1786 - accuracy: 0.9331 - val_loss: 0.3547 - val_accuracy: 0.8942\n","Epoch 18/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1744 - accuracy: 0.9356 - val_loss: 0.3364 - val_accuracy: 0.8940\n","Epoch 19/50\n","1500/1500 [==============================] - 10s 6ms/step - loss: 0.1679 - accuracy: 0.9367 - val_loss: 0.3372 - val_accuracy: 0.8930\n","Epoch 20/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1623 - accuracy: 0.9393 - val_loss: 0.3503 - val_accuracy: 0.8871\n","Epoch 21/50\n","1500/1500 [==============================] - 10s 6ms/step - loss: 0.1577 - accuracy: 0.9405 - val_loss: 0.3727 - val_accuracy: 0.8885\n","Epoch 22/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1535 - accuracy: 0.9422 - val_loss: 0.3696 - val_accuracy: 0.8892\n","Epoch 23/50\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.1494 - accuracy: 0.9435 - val_loss: 0.3575 - val_accuracy: 0.8924\n","Epoch 24/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1430 - accuracy: 0.9463 - val_loss: 0.3504 - val_accuracy: 0.8972\n","Epoch 25/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1389 - accuracy: 0.9478 - val_loss: 0.3653 - val_accuracy: 0.8932\n","Epoch 26/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1375 - accuracy: 0.9475 - val_loss: 0.3671 - val_accuracy: 0.8942\n","Epoch 27/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1306 - accuracy: 0.9509 - val_loss: 0.3772 - val_accuracy: 0.8925\n","Epoch 28/50\n","1500/1500 [==============================] - 10s 6ms/step - loss: 0.1299 - accuracy: 0.9511 - val_loss: 0.4027 - val_accuracy: 0.8886\n","Epoch 29/50\n","1500/1500 [==============================] - 11s 7ms/step - loss: 0.1248 - accuracy: 0.9531 - val_loss: 0.3942 - val_accuracy: 0.8954\n","Epoch 30/50\n","1500/1500 [==============================] - 10s 7ms/step - loss: 0.1243 - accuracy: 0.9536 - val_loss: 0.4041 - val_accuracy: 0.8957\n","Epoch 31/50\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.1178 - accuracy: 0.9561 - val_loss: 0.3909 - val_accuracy: 0.8955\n","Epoch 32/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1155 - accuracy: 0.9569 - val_loss: 0.3873 - val_accuracy: 0.8972\n","Epoch 33/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1154 - accuracy: 0.9558 - val_loss: 0.4255 - val_accuracy: 0.8909\n","Epoch 34/50\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.1092 - accuracy: 0.9593 - val_loss: 0.4326 - val_accuracy: 0.8932\n","Epoch 35/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1082 - accuracy: 0.9595 - val_loss: 0.4227 - val_accuracy: 0.8937\n","Epoch 36/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1041 - accuracy: 0.9607 - val_loss: 0.4471 - val_accuracy: 0.8955\n","Epoch 37/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1040 - accuracy: 0.9620 - val_loss: 0.4370 - val_accuracy: 0.8953\n","Epoch 38/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1015 - accuracy: 0.9614 - val_loss: 0.4411 - val_accuracy: 0.8943\n","Epoch 39/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.0973 - accuracy: 0.9637 - val_loss: 0.4790 - val_accuracy: 0.8903\n","Epoch 40/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.0994 - accuracy: 0.9624 - val_loss: 0.4658 - val_accuracy: 0.8937\n","Epoch 41/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.0943 - accuracy: 0.9648 - val_loss: 0.4553 - val_accuracy: 0.8979\n","Epoch 42/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.0924 - accuracy: 0.9654 - val_loss: 0.4703 - val_accuracy: 0.8928\n","Epoch 43/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.0915 - accuracy: 0.9663 - val_loss: 0.4907 - val_accuracy: 0.8912\n","Epoch 44/50\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.0912 - accuracy: 0.9663 - val_loss: 0.4925 - val_accuracy: 0.8937\n","Epoch 45/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.0878 - accuracy: 0.9669 - val_loss: 0.4750 - val_accuracy: 0.8964\n","Epoch 46/50\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.0840 - accuracy: 0.9693 - val_loss: 0.5047 - val_accuracy: 0.8897\n","Epoch 47/50\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.0840 - accuracy: 0.9686 - val_loss: 0.5294 - val_accuracy: 0.8896\n","Epoch 48/50\n","1500/1500 [==============================] - 10s 6ms/step - loss: 0.0828 - accuracy: 0.9691 - val_loss: 0.4950 - val_accuracy: 0.8939\n","Epoch 49/50\n","1500/1500 [==============================] - 10s 6ms/step - loss: 0.0785 - accuracy: 0.9715 - val_loss: 0.5161 - val_accuracy: 0.8919\n","Epoch 50/50\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.0805 - accuracy: 0.9703 - val_loss: 0.5297 - val_accuracy: 0.8896\n","Best epoch: 41\n"]}]},{"cell_type":"markdown","source":["# Retrain the model"],"metadata":{"id":"bGRmO5lNw3lY"}},{"cell_type":"code","source":["hypermodel = tuner.hypermodel.build(best_hps)\n","hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FgGoqSWgrwHc","executionInfo":{"status":"ok","timestamp":1713789776146,"user_tz":-330,"elapsed":383374,"user":{"displayName":"Tushar Pandey","userId":"02510744473828278127"}},"outputId":"c68960c4-00e1-4269-9d49-af269d5c45fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.5081 - accuracy: 0.8209 - val_loss: 0.4077 - val_accuracy: 0.8584\n","Epoch 2/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.3764 - accuracy: 0.8628 - val_loss: 0.3922 - val_accuracy: 0.8583\n","Epoch 3/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.3355 - accuracy: 0.8765 - val_loss: 0.3554 - val_accuracy: 0.8743\n","Epoch 4/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.3115 - accuracy: 0.8842 - val_loss: 0.3923 - val_accuracy: 0.8482\n","Epoch 5/41\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.2914 - accuracy: 0.8922 - val_loss: 0.3424 - val_accuracy: 0.8797\n","Epoch 6/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2772 - accuracy: 0.8961 - val_loss: 0.3207 - val_accuracy: 0.8833\n","Epoch 7/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2637 - accuracy: 0.9015 - val_loss: 0.3352 - val_accuracy: 0.8792\n","Epoch 8/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.2514 - accuracy: 0.9062 - val_loss: 0.3271 - val_accuracy: 0.8849\n","Epoch 9/41\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.2419 - accuracy: 0.9090 - val_loss: 0.3209 - val_accuracy: 0.8875\n","Epoch 10/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2296 - accuracy: 0.9124 - val_loss: 0.3169 - val_accuracy: 0.8864\n","Epoch 11/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.2200 - accuracy: 0.9175 - val_loss: 0.3178 - val_accuracy: 0.8909\n","Epoch 12/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.2133 - accuracy: 0.9191 - val_loss: 0.3307 - val_accuracy: 0.8888\n","Epoch 13/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.2043 - accuracy: 0.9223 - val_loss: 0.3194 - val_accuracy: 0.8921\n","Epoch 14/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.1982 - accuracy: 0.9256 - val_loss: 0.3226 - val_accuracy: 0.8923\n","Epoch 15/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.1931 - accuracy: 0.9286 - val_loss: 0.3214 - val_accuracy: 0.8932\n","Epoch 16/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1844 - accuracy: 0.9314 - val_loss: 0.3309 - val_accuracy: 0.8922\n","Epoch 17/41\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.1782 - accuracy: 0.9330 - val_loss: 0.3454 - val_accuracy: 0.8889\n","Epoch 18/41\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.1742 - accuracy: 0.9358 - val_loss: 0.3309 - val_accuracy: 0.8963\n","Epoch 19/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1686 - accuracy: 0.9383 - val_loss: 0.3489 - val_accuracy: 0.8937\n","Epoch 20/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1617 - accuracy: 0.9402 - val_loss: 0.3302 - val_accuracy: 0.8942\n","Epoch 21/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.1572 - accuracy: 0.9413 - val_loss: 0.3604 - val_accuracy: 0.8916\n","Epoch 22/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1531 - accuracy: 0.9421 - val_loss: 0.3505 - val_accuracy: 0.8910\n","Epoch 23/41\n","1500/1500 [==============================] - 10s 6ms/step - loss: 0.1501 - accuracy: 0.9442 - val_loss: 0.3838 - val_accuracy: 0.8912\n","Epoch 24/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1451 - accuracy: 0.9451 - val_loss: 0.3542 - val_accuracy: 0.8972\n","Epoch 25/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1417 - accuracy: 0.9466 - val_loss: 0.3769 - val_accuracy: 0.8916\n","Epoch 26/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1366 - accuracy: 0.9486 - val_loss: 0.3931 - val_accuracy: 0.8928\n","Epoch 27/41\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.1350 - accuracy: 0.9489 - val_loss: 0.3689 - val_accuracy: 0.8927\n","Epoch 28/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.1306 - accuracy: 0.9518 - val_loss: 0.3777 - val_accuracy: 0.8923\n","Epoch 29/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1226 - accuracy: 0.9538 - val_loss: 0.3995 - val_accuracy: 0.8918\n","Epoch 30/41\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.1236 - accuracy: 0.9535 - val_loss: 0.3917 - val_accuracy: 0.8923\n","Epoch 31/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1202 - accuracy: 0.9561 - val_loss: 0.3999 - val_accuracy: 0.8948\n","Epoch 32/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1161 - accuracy: 0.9571 - val_loss: 0.3862 - val_accuracy: 0.8971\n","Epoch 33/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1121 - accuracy: 0.9582 - val_loss: 0.4133 - val_accuracy: 0.8921\n","Epoch 34/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.1125 - accuracy: 0.9572 - val_loss: 0.4180 - val_accuracy: 0.8878\n","Epoch 35/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1081 - accuracy: 0.9590 - val_loss: 0.4336 - val_accuracy: 0.8934\n","Epoch 36/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1067 - accuracy: 0.9607 - val_loss: 0.4107 - val_accuracy: 0.8956\n","Epoch 37/41\n","1500/1500 [==============================] - 8s 6ms/step - loss: 0.1021 - accuracy: 0.9617 - val_loss: 0.4369 - val_accuracy: 0.8982\n","Epoch 38/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.1025 - accuracy: 0.9613 - val_loss: 0.4140 - val_accuracy: 0.8952\n","Epoch 39/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.0980 - accuracy: 0.9633 - val_loss: 0.4278 - val_accuracy: 0.8913\n","Epoch 40/41\n","1500/1500 [==============================] - 8s 5ms/step - loss: 0.0959 - accuracy: 0.9642 - val_loss: 0.4876 - val_accuracy: 0.8867\n","Epoch 41/41\n","1500/1500 [==============================] - 9s 6ms/step - loss: 0.0967 - accuracy: 0.9647 - val_loss: 0.4343 - val_accuracy: 0.9003\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x78bacce3d300>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["eval_result = hypermodel.evaluate(img_test, label_test)\n","print(\"[test loss, test accuracy]:\", eval_result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cn2b-hgNxiGB","executionInfo":{"status":"ok","timestamp":1713789777944,"user_tz":-330,"elapsed":1810,"user":{"displayName":"Tushar Pandey","userId":"02510744473828278127"}},"outputId":"281abff3-8c3c-4bb7-b66f-5bdf43a7b5de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 3ms/step - loss: 0.5000 - accuracy: 0.8911\n","[test loss, test accuracy]: [0.5000040531158447, 0.8910999894142151]\n"]}]}]}